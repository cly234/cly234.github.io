<!DOCTYPE HTML>
<html lang="en">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Chunlin Yu</title>
  <meta name="author" content="Chunlin Yu">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      color: #333;
    }
    table {
      width: 100%;
      max-width: 800px;
      border-collapse: separate;
      margin: auto;
    }
    .content-cell {
      padding: 20px;
      vertical-align: middle;
    }
    .text-center {
      text-align: center;
    }
    .papertitle {
      font-weight: bold;
      color: #0056b3;
    }
    .one img {
      width: 100%;
      height: auto;
      display: block;
      object-fit: cover;
    }
    .table-cell {
      display: flex;
      align-items: center;
      justify-content: center;
    }
    .table-cell img {
      max-width: 100%;
      height: auto;
    }
    .name {
      font-size: 1.5em;
      margin: 0;
    }
    p {
      margin: 0 0 1em 0;
    }
    a {
      color: #0056b3;
      text-decoration: none;
    }
    a:hover {
      text-decoration: underline;
    }
    h2 {
      margin: 0 0 1em 0;
    }
    .footer-text {
      text-align: right;
      font-size: small;
      margin-top: 20px;
    }
  </style>
</head>
<body>
  <table>
    <tbody>
      <!-- Personal Information -->
      <tr>
        <td class="content-cell">
          <table>
            <tbody>
              <tr>
                <td class="content-cell" style="width:63%; vertical-align:middle;">
                  <p class="name text-center">
                    Chunlin Yu
                  </p>
                  <p>I'm a second-year master's student at <a href="https://shanghaitech.edu.cn">ShanghaiTech</a> University, where I am currently advised by <a href="https://faculty.sist.shanghaitech.edu.cn/faculty/wangjingya/">Jingya Wang</a>.</p>
                  <p>Before pursuing the master's degree, I obtained my bachelor's degree from ShanghaiTech University. I've received the Outstanding Student Award in 2023.</p>
                  <p class="text-center">
                    <a href="mailto:yuchl2022@shanghaitech.edu.cn">Email</a> &nbsp;/&nbsp;
                    <a href="data/Resume.pdf">CV</a> &nbsp;/&nbsp;
                    <a href="https://scholar.google.com/citations?hl=zh-CN&user=DUOBXBgAAAAJ">Google Scholar</a> &nbsp;/&nbsp;
                    <a href="https://github.com/cly234">Github</a>
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
          <!-- Research -->
          <table>
            <tbody>
              <tr>
                <td class="content-cell">
                  <h2>Research</h2>
                  <p>My research interests include self-supervised learning and human-centered vision, including but not limited to LLM/LVM, deep clustering, biometric recognition, 2D/3D human-object interaction, and affordances.</p>
                </td>
              </tr>
            </tbody>
          </table>
          <!-- Papers -->
          <table>
            <tbody>
              <!-- Paper 1 -->
              <tr>
                <td class="table-cell" style="width:25%;">
                  <div class="one">
                    <img src='images/aaai2024.png' alt='HybridGait Paper'>
                  </div>
                </td>
                <td class="content-cell" style="width:75%;">
                  <a href="https://arxiv.org/abs/2401.00271">
                    <span class="papertitle">HybridGait: A Benchmark for Spatial-Temporal Cloth-Changing Gait Recognition with Hybrid Explorations</span>
                  </a>
                  <br>
                  Yilan Dong, <strong>Chunlin Yu</strong>, Ruiyang Ha, Ye Shi, Yuexin Ma, Yanwei Fu, Jingya Wang
                  <br>
                  <em>AAAI</em>, 2024
                  <br>
                  <a href="cly234.github.io">Paper</a> / <a href="cly234.github.io">Code</a>
                  <p>A challenging benchmark CCGait that captures realistic appearance changes over expanded time and space, as well as a hybrid framework HybridGait.</p>
                </td>
              </tr>
              <!-- Paper 2 -->
              <tr>
                <td class="table-cell" style="width:25%;">
                  <div class="one">
                    <img src='images/nips23.png' alt='Contextually Affinitive Paper'>
                  </div>
                </td>
                <td class="content-cell" style="width:75%;">
                  <a href="https://openreview.net/pdf?id=Psj0jHocm1/">
                    <span class="papertitle">Contextually Affinitive Neighborhood Refinery for Deep Clustering</span>
                  </a>
                  <br>
                  <strong>Chunlin Yu</strong>, Ye Shi, Jingya Wang
                  <br>
                  <em>NeurIPS</em>, 2023
                  <br>
                  <a href="https://openreview.net/pdf?id=Psj0jHocm1">Paper</a> / <a href="https://github.com/cly234/DeepClustering-ConNR">Code</a>
                  <p>We propose ConAff Neighborhoods for more context-rich neighbor retrievals as well as a progressive Boundary Filtering strategy for noise-resilient neighborhoods.</p>
                </td>
              </tr>
              <!-- Paper 3 -->
              <tr>
                <td class="table-cell" style="width:25%;">
                  <div class="one">
                    <img src='images/aaai2023.png' alt='Lifelong Person Re-Identification Paper'>
                  </div>
                </td>
                <td class="content-cell" style="width:75%;">
                  <a href="https://cly234.github.io/KRKC-projectpage/">
                    <span class="papertitle">Lifelong Person Re-Identification via Knowledge Refreshing and Consolidation</span>
                  </a>
                  <br>
                  <strong>Chunlin Yu</strong>, Ye Shi, Zimo Liu, Shenghua Gao, Jingya Wang
                  <br>
                  <em>AAAI</em>, 2023
                  <br>
                  <a href="https://cly234.github.io/KRKC-projectpage/">Project Page</a> / <a href="https://arxiv.org/abs/2211.16201">Paper</a>
                  <p>Using a biological-inspired network involving a dynamical memory model and an adaptive working model to refresh and consolidate the knowledge on the fly.</p>
                </td>
              </tr>
              <!-- Paper 4 -->
              <tr>
                <td class="table-cell" style="width:25%;">
                  <div class="one">
                    <img src='images/oh-former-v3.png' alt='Oh-Former Paper'>
                  </div>
                </td>
                <td class="content-cell" style="width:75%;">
                  <a href="https://arxiv.org/abs/2109.11159">
                    <span class="papertitle">Oh-Former: Omni-Relational High-Order Transformer for Person Re-Identification</span>
                  </a>
                  <br>
                  Xianing Chen*, <strong>Chunlin Yu*</strong>, Qiong Cao, Jialang Xu, Yujie Zhong, Jiale Xu, Zhengxin Li, Jingya Wang, Shenghua Gao
                  <br>
                  <em>arXiv</em>, 2021
                  <br>
                  <p>A high-order transformer that captures omni-relational information for person re-identification.</p>
                </td>
              </tr>
            </tbody>
          </table>
          <!-- Services -->
          <table>
            <tbody>
              <tr>
                <td class="content-cell">
                  <h2>Services</h2>
                </td>
              </tr>
            </tbody>
          </table>
          <table>
            <tbody>
              <tr>
                <td class="content-cell">
                  Conference Reviewers: <a href="https://cvpr.thecvf.com/Conferences/2024/Organizers">CVPR</a>, <a href="https://eccv2024.org/organizing-committee/">ECCV</a>, <a href="https://www.iccv2023.org/committee/">ICCV</a>, <a href="https://www.iccv2023.org/committee/">ICCV</a>, <a href="https://iclr.cc/Conferences/2024/Committee">ICLR</a>
                </td>
              </tr>
            </tbody>
          </table>
        </td>
      </tr>
    </tbody>
  </table>
</body>
</html>
