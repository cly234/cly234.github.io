<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <title>Chunlin Yu</title>
    <meta name="author" content="Chunlin Yu">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>
  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Chunlin Yu
                </p>
                <p>I'm a second-year master's student at <a href="shanghaitech.edu.cn">ShanghaiTech</a> University, where I am currently advised by <a href="https://faculty.sist.shanghaitech.edu.cn/faculty/wangjingya/">Jingya Wang</a>.
                </p>
                <p>
                  Before pursuing the master's degree, I obtained my bachelor's degree from ShanghaiTech University. I've received the Outstanding Student Award in 2023. 
                </p>
                <p style="text-align:center">
                  <a href="yuchl2022@shanghaitech.edu.cn">Email</a> &nbsp;/&nbsp;
                  <a href="data/Resume.pdf">CV</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?hl=zh-CN&user=DUOBXBgAAAAJ">Google Scholar</a> &nbsp;/&nbsp;
                  <a href="https://github.com/cly234">Github</a>
                </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <p>
                  My research interests include self-supervised learning and human-centered vision, including but not limited to LLM/LVM, deep clustering, biometric recognition, 2D/3D human-object interaction, and affordances.
                </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
		  
    	<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/aaai2024.png' width=100%>
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2401.00271">
                <span class="papertitle">HybridGait: A Benchmark for Spatial-Temporal Cloth-Changing Gait Recognition with Hybrid Explorations</span>
              </a>
              <br>
              Yilan Dong,
        	<strong>Chunlin Yu</strong>,
		Ruiyang Ha,
       		Ye Shi,
		Yuexin Ma,
		Yanwei Fu,
        	Jingya Wang,
              <br>
              <em>AAAI</em>, 2024
       	      <br>
	      <a href="cly234.github.io">Paper</a>
              /
              <a href="cly234.github.io">Code</a>
              <p></p>
               <p>
        	A challenging benchmark CCGait that captures realistic appearance changes over expanded time and space, as well as a hybrid framework HybridGait.
        	</p>
            </td>
        </tr>
    <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/nips23.png' width=100%>
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://openreview.net/pdf?id=Psj0jHocm1/">
                <span class="papertitle">Contextually Affinitive Neighborhood Refinery for Deep Clustering</span>
              </a>
        <br>
        <strong>Chunlin Yu</strong>,
        Ye Shi,
        Jingya Wang,
        <br>
        <em>NeurIPS</em>, 2023
        <br>
	<a href="https://openreview.net/pdf?id=Psj0jHocm1">Paper</a>
        /
        <a href="https://github.com/cly234/DeepClustering-ConNR">Code</a>
        <p></p>
        <p>
        We propose ConAff Neighborhoods for more context-rich neighbor retrievals as well as a progressive Boundary Filtering strategy for noise-resilient neighborhoods.
        </p>
      </td>
    </tr>
	
	
    <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
                <img src='images/aaai2023.png' width=100%>
            </div>
        </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://cly234.github.io/KRKC-projectpage/">
                <span class="papertitle">Lifelong Person Re-Identification via Knowledge Refreshing and Consolidation</span>
              </a>
        <br>
        <strong>Chunlin Yu</strong>,
	Ye Shi,
	Zimo Liu,
	Shenghua Gao,
	Jingya Wang
        <br>
        <em>AAAI</em>, 2023
        <br>
        <a href="https://cly234.github.io/KRKC-projectpage/">project page</a>
        /
        <a href="https://arxiv.org/abs/2211.16201">Paper</a>
        <p></p>
        <p>
        Using a biological-inspired network involving a dynamical memory model and an adaptive working model to refresh and consolidate the knowledge on the fly.
        </p>
      </td>
    </tr>
<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/oh-former-v3.png' width=100%>
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
    <a href="https://arxiv.org/abs/2109.11159">
      <span class="papertitle">Oh-Former: Omni-Relational High-Order Transformer for Person Re-Identification
</span>
    </a>
    <br>
	Xianing Chen*, 
	<strong>Chunlin Yu*</strong>,
	Qiong Cao, 
	Jialang Xu, 
	Yujie Zhong, 
	Jiale Xu, 
	Zhengxin Li, 
	Jingya Wang, 
	Shenghua Gao
    <br>
	<em>arXiv<em>, 2021
    <br>
    <p></p>
    <p>
    A high-order transformer that captures omni-relational information for person re-identification.
    </p>
  </td>
</tr>         
          </tbody></table>
          
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
            <tr>
              <td>
                <h2>Services</h2>
              </td>
            </tr>
          </tbody></table>
          <table width="100%" align="center" border="0" cellpadding="20"><tbody>
            
            <tr>
              <td width="75%" valign="center">
                Conference Reviewers:<a href="https://cvpr.thecvf.com/Conferences/2024/Organizers">CVPR 2024</a>, <a href="https://iclr.cc/">ICLR 2024</a>
                <br>
                Journal Reviewers: TNNLS 2022, TMM 2023
                <br>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                  Fored from <a href="https://github.com/jonbarron/jonbarron_website">jonbarron's website</a>.
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
