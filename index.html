<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <title>Chunlin Yu</title>
    <meta name="author" content="Chunlin Yu">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
  </head>
  <body>
    <div style="max-width:800px; margin:auto; padding: 0;">
      <div style="padding:2.5%; text-align: center;">
        <p class="name">Chunlin Yu</p>
        <p>I'm a second-year master's student at <a href="shanghaitech.edu.cn">ShanghaiTech</a> University, where I am currently advised by <a href="https://faculty.sist.shanghaitech.edu.cn/faculty/wangjingya/">Jingya Wang</a>.</p>
        <p>Before pursuing the master's degree, I obtained my bachelor's degree from ShanghaiTech University.</p>
        <p>
          <a href="yuchl2022@shanghaitech.edu.cn">Email</a> &nbsp;/&nbsp;
          <a href="images/Resume.pdf">CV</a> &nbsp;/&nbsp;
          <a href="https://scholar.google.com/citations?hl=zh-CN&user=PoCQ9LUAAAAJ">Google Scholar</a> &nbsp;/&nbsp;
          <a href="https://github.com/cly234">Github</a>
        </p>
      </div>
      
      <div style="padding: 20px;">
        <h2>Research</h2>
        <p>My research interests include self-supervised learning and human-centered vision, including but not limited to LLM/LVM, deep clustering, biometric recognition, 2D/3D human-object interaction, and affordances.</p>
      </div>

      <div style="display: flex; padding: 10px;">
        <div style="flex: 1; padding-right: 10px;">
          <img src='images/aaai2024.png' style="width:100%; height:auto;">
        </div>
        <div style="flex: 2;">
          <a href="https://arxiv.org/abs/2401.00271">
            <span class="papertitle">HybridGait: A Benchmark for Spatial-Temporal Cloth-Changing Gait Recognition with Hybrid Explorations</span>
          </a>
          <br>
          Yilan Dong, <strong>Chunlin Yu</strong>, Ruiyang Ha, Ye Shi, Yuexin Ma, Yanwei Fu, Jingya Wang,
          <br>
          <em>AAAI</em>, 2024
          <br>
          <a href="cly234.github.io">Paper</a> /
          <a href="cly234.github.io">Code</a>
          <p>A challenging benchmark CCGait that captures realistic appearance changes over expanded time and space, as well as a hybrid framework HybridGait.</p>
        </div>
      </div>

      <div style="display: flex; padding: 10px;">
        <div style="flex: 1; padding-right: 10px;">
          <img src='images/nips23.png' style="width:100%; height:auto;">
        </div>
        <div style="flex: 2;">
          <a href="https://openreview.net/pdf?id=Psj0jHocm1/">
            <span class="papertitle">Contextually Affinitive Neighborhood Refinery for Deep Clustering</span>
          </a>
          <br>
          <strong>Chunlin Yu</strong>, Ye Shi, Jingya Wang,
          <br>
          <em>NeurIPS</em>, 2023
          <br>
          <a href="https://openreview.net/pdf?id=Psj0jHocm1">Paper</a> /
          <a href="https://github.com/cly234/DeepClustering-ConNR">Code</a>
          <p>We propose ConAff Neighborhoods for more context-rich neighbor retrievals as well as a progressive Boundary Filtering strategy for noise-resilient neighborhoods.</p>
        </div>
      </div>

      <div style="display: flex; padding: 10px;">
        <div style="flex: 1; padding-right: 10px;">
          <img src='images/aaai2023.png' style="width:100%; height:auto;">
        </div>
        <div style="flex: 2;">
          <a href="https://cly234.github.io/KRKC-projectpage/">
            <span class="papertitle">Lifelong Person Re-Identification via Knowledge Refreshing and Consolidation</span>
          </a>
          <br>
          <strong>Chunlin Yu</strong>, Ye Shi, Zimo Liu, Shenghua Gao, Jingya Wang
          <br>
          <em>AAAI</em>, 2023
          <br>
          <a href="https://cly234.github.io/KRKC-projectpage/">project page</a> /
          <a href="https://arxiv.org/abs/2211.16201">Paper</a>
          <p>Using a biological-inspired network involving a dynamical memory model and an adaptive working model to refresh and consolidate the knowledge on the fly.</p>
        </div>
      </div>

      <div style="display: flex; padding: 10px;">
        <div style="flex: 1; padding-right: 10px;">
          <img src='images/oh-former-v3.png' style="width:100%; height:auto;">
        </div>
        <div style="flex: 2;">
          <a href="https://arxiv.org/abs/2109.11159">
            <span class="papertitle">Oh-Former: Omni-Relational High-Order Transformer for Person Re-Identification</span>
          </a>
          <br>
          Xianing Chen*, <strong>Chunlin Yu*</strong>, Qiong Cao, Jialang Xu, Yujie Zhong, Jiale Xu, Zhengxin Li, Jingya Wang, Shenghua Gao
          <br>
          <em>arXiv</em>, 2021
          <p>A high-order transformer that captures omni-relational information for person re-identification.</p>
        </div>
      </div>

      <div style="padding: 20px;">
        <h2>Services</h2>
        <p>Conference Reviewers: <a href="https://cvpr.thecvf.com/Conferences/2024/Organizers">CVPR 2024</a>, <a href="https://iclr.cc/">ICLR 2024</a></p>
        <p>Journal Reviewers: TNNLS 2022, TMM 2023</p>
      </div>
      
      <div style="text-align:right;font-size:small;">
        <p>Fored from <a href="https://github.com/jonbarron/jonbarron_website">jonbarron's website</a>.</p>
      </div>
    </div>
  </body>
</html>
